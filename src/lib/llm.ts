/**
 * ================================
 * LLM Service - Sylion Backend
 * ================================
 * 
 * Service stub pour la g√©n√©ration de r√©ponses IA.
 * √Ä remplacer par Vertex AI dans une prochaine phase.
 */

import { logger } from '@/lib/logger';
import { assistantService } from '@/modules/assistant/assistant.service';

/**
 * ================================
 * Interface du service LLM
 * ================================
 */

export interface LLMMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface GenerateReplyOptions {
  tenantId: string;
  assistantId: string;
  messages: LLMMessage[];
  maxTokens?: number;
  temperature?: number;
}

export interface LLMResponse {
  content: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model: string;
  timestamp: string;
}

/**
 * ================================
 * Service LLM Stub
 * ================================
 */

/**
 * G√©n√©rer une r√©ponse d'assistant IA (version stub)
 */
export async function generateAssistantReply(
  options: GenerateReplyOptions
): Promise<string> {
  try {
    logger.info('Generating assistant reply', {
      tenantId: options.tenantId,
      assistantId: options.assistantId,
      messagesCount: options.messages.length,
    });

    // R√©cup√©rer la configuration de l'assistant
    const assistant = await assistantService.getAssistantById(options.assistantId);
    if (!assistant) {
      throw new Error(`Assistant not found: ${options.assistantId}`);
    }

    // Analyser le dernier message utilisateur
    const lastUserMessage = options.messages
      .filter(msg => msg.role === 'user')
      .pop();

    if (!lastUserMessage) {
      throw new Error('No user message found in conversation');
    }

    // G√©n√©ration stub bas√©e sur des r√®gles simples
    const reply = await generateStubReply(lastUserMessage.content, assistant, options);

    logger.info('Assistant reply generated successfully', {
      tenantId: options.tenantId,
      assistantId: options.assistantId,
      userMessageLength: lastUserMessage.content.length,
      replyLength: reply.length,
    });

    return reply;

  } catch (error) {
    logger.error('Error generating assistant reply', {
      tenantId: options.tenantId,
      assistantId: options.assistantId,
      error: error instanceof Error ? error.message : String(error),
    });
    throw error;
  }
}

/**
 * ================================
 * G√©n√©rateur Stub
 * ================================
 */

async function generateStubReply(
  userMessage: string,
  assistant: any,
  options: GenerateReplyOptions
): Promise<string> {
  
  const lowerMessage = userMessage.toLowerCase();

  // R√©ponses pr√©d√©finies pour certains motifs courants
  const patterns = [
    {
      keywords: ['bonjour', 'salut', 'hello', 'hi', 'bonsoir'],
      responses: [
        `Bonjour ! Je suis ${assistant.name}, votre assistant virtuel. Comment puis-je vous aider aujourd'hui ?`,
        `Salut ! C'est ${assistant.name}. En quoi puis-je vous √™tre utile ?`,
        `Hello ! Je suis l√† pour vous aider. Que puis-je faire pour vous ?`,
      ]
    },
    {
      keywords: ['au revoir', 'bye', '√† bient√¥t', 'tchao'],
      responses: [
        'Au revoir ! N\'h√©sitez pas √† revenir si vous avez d\'autres questions.',
        '√Ä bient√¥t ! J\'esp√®re avoir pu vous aider.',
        'Bonne journ√©e ! Je reste √† votre disposition.',
      ]
    },
    {
      keywords: ['merci', 'thank you', 'thanks'],
      responses: [
        'Je vous en prie ! C\'√©tait un plaisir de vous aider.',
        'Avec plaisir ! N\'h√©sitez pas si vous avez d\'autres questions.',
        'De rien ! Je suis l√† pour √ßa.',
      ]
    },
    {
      keywords: ['aide', 'help', 'comment', 'que faire'],
      responses: [
        'Je suis votre assistant virtuel et je peux vous aider avec diverses questions. Pouvez-vous √™tre plus pr√©cis sur ce dont vous avez besoin ?',
        'Je suis l√† pour vous accompagner ! Dites-moi ce que vous cherchez et je ferai de mon mieux pour vous aider.',
        'Bien s√ªr, je peux vous aider ! Pouvez-vous me dire exactement ce que vous souhaitez savoir ?',
      ]
    },
    {
      keywords: ['prix', 'tarif', 'co√ªt', 'combien'],
      responses: [
        'Pour les informations de tarifs, je vous recommande de contacter notre √©quipe commerciale qui pourra vous donner tous les d√©tails.',
        'Les tarifs varient selon vos besoins sp√©cifiques. Souhaitez-vous que je vous mette en relation avec un conseiller ?',
        'Je n\'ai pas acc√®s aux tarifs exacts, mais notre √©quipe peut vous fournir un devis personnalis√©.',
      ]
    },
    {
      keywords: ['probl√®me', 'bug', 'erreur', 'ne marche pas'],
      responses: [
        'Je suis d√©sol√© d\'apprendre que vous rencontrez un probl√®me. Pouvez-vous me d√©crire plus pr√©cis√©ment ce qui ne fonctionne pas ?',
        'C\'est ennuyeux ! Pourriez-vous me donner plus de d√©tails sur le probl√®me que vous rencontrez ?',
        'Je vais essayer de vous aider. Pouvez-vous me d√©crire √©tape par √©tape ce qui s\'est pass√© ?',
      ]
    }
  ];

  // Recherche de motifs correspondants
  for (const pattern of patterns) {
    const hasKeyword = pattern.keywords.some(keyword => 
      lowerMessage.includes(keyword)
    );
    
    if (hasKeyword) {
      const randomResponse = pattern.responses[
        Math.floor(Math.random() * pattern.responses.length)
      ];
      return randomResponse || 'Merci pour votre message, je vais vous aider !';
    }
  }

  // R√©ponses par d√©faut bas√©es sur la longueur du message
  if (userMessage.length > 200) {
    return 'Merci pour votre message d√©taill√©. J\'ai bien pris note de vos informations. Un membre de notre √©quipe reviendra vers vous avec une r√©ponse compl√®te sous peu.';
  }

  if (userMessage.length < 10) {
    return 'Pourriez-vous me donner un peu plus de d√©tails ? Je suis l√† pour vous aider de mon mieux !';
  }

  // R√©ponse g√©n√©rique intelligente
  const genericResponses = [
    'C\'est une excellente question ! Laissez-moi voir comment je peux vous aider avec cela.',
    'Je comprends votre demande. Permettez-moi de vous orienter vers la meilleure solution.',
    'Merci pour votre question. Je vais faire de mon mieux pour vous donner une r√©ponse utile.',
    'Int√©ressant ! Pouvez-vous me donner un peu plus de contexte pour que je puisse mieux vous aider ?',
    'Je prends note de votre demande. Voici ce que je peux vous dire √† ce sujet...',
  ];

  const randomGeneric = genericResponses[
    Math.floor(Math.random() * genericResponses.length)
  ];

  // Ajouter une note sur le fait que c'est un assistant en d√©veloppement
  const devNote = '\n\nüí° *Note: Je suis encore en d√©veloppement et mes capacit√©s s\'am√©lioreront bient√¥t avec l\'IA avanc√©e !*';
  
  return randomGeneric + devNote;
}

/**
 * ================================
 * Fonctions utilitaires
 * ================================
 */

/**
 * Valider la configuration de l'assistant pour l'IA
 */
export async function validateAssistantForLLM(assistantId: string): Promise<boolean> {
  try {
    const assistant = await assistantService.getAssistantById(assistantId);
    
    if (!assistant || !assistant.isActive) {
      return false;
    }

    // V√©rifications sp√©cifiques pour l'IA
    const config = assistant;
    
    if (!config.systemPrompt || config.systemPrompt.length < 10) {
      logger.warn('Assistant has insufficient system prompt', { assistantId });
      // Ne pas faire √©chouer, utiliser un prompt par d√©faut
    }

    const temperature = typeof config.temperature === 'string' 
      ? parseFloat(config.temperature) 
      : config.temperature;
      
    if (temperature && (temperature < 0 || temperature > 2)) {
      logger.warn('Assistant has invalid temperature setting', { 
        assistantId, 
        temperature: config.temperature 
      });
    }

    return true;

  } catch (error) {
    logger.error('Error validating assistant for LLM', {
      assistantId,
      error: error instanceof Error ? error.message : String(error),
    });
    return false;
  }
}

/**
 * Obtenir les m√©triques d'utilisation LLM (stub)
 */
export async function getLLMUsageStats(tenantId: string): Promise<{
  requestsToday: number;
  tokensUsedToday: number;
  averageResponseTime: number;
}> {
  // TODO: Impl√©menter les vraies m√©triques avec Redis ou DB
  return {
    requestsToday: Math.floor(Math.random() * 100),
    tokensUsedToday: Math.floor(Math.random() * 10000),
    averageResponseTime: Math.floor(Math.random() * 2000) + 500, // 500-2500ms
  };
}

/**
 * Pr√©paration pour l'int√©gration future Vertex AI
 */
export interface VertexAIConfig {
  projectId: string;
  location: string;
  model: string;
  maxTokens: number;
  temperature: number;
  topK?: number;
  topP?: number;
}

/**
 * Stub pour la future int√©gration Vertex AI
 */
export async function generateWithVertexAI(
  config: VertexAIConfig,
  messages: LLMMessage[]
): Promise<LLMResponse> {
  // TODO: Impl√©menter l'appel r√©el √† Vertex AI
  throw new Error('Vertex AI integration not implemented yet. Use generateAssistantReply() for now.');
}

/**
 * Fonction de migration pour passer du stub vers Vertex AI
 */
export async function migrateToVertexAI(): Promise<void> {
  logger.info('Vertex AI migration not yet implemented. Current: stub mode.');
  // TODO: Logique de migration progressive stub -> Vertex AI
}